ls()
getwd()
load("~/VPS/prism-los-repo/Data_2009Q1_2014Q2/data_part2.rda")
### My answer is $49,086.69.
### I used multiple regression to predict ice cream sales from temperature and day of week
### because the relationship between the two variables seems to be linear.
library(stringr)
### Step 1: Data Preprocessing
###    		a. Convert the Excel file to CSV and convert the sales numbers to be numeric:
data <- read.csv("C:\\Users\\ikukuyeva\\Documents\\Resume\\Resume-repo\\Ice Cream Data.csv")
### 		b. Convert the Temperature variable to be numeric:
data[, 2] <- str_replace_all(data$Temperature, "F", "")
data[, 2] <- as.numeric(data[, 2])
###			c. Add a day of the week component
tmp_date    <- do.call(rbind, str_split(data$Date, ","))
day_of_week <- tmp_date[, 1]
### Step 2: Fit a multiple regression model:
fit <- lm(Ice.Cream.Sales~Temperature+as.factor(day_of_week), data=data)
predict(fit, data.frame(Temperature=65,
day_of_week="Sunday")
) # 49086.69
### Step 3: Plot the model fit:
plot(data$Temperature, data$Ice.Cream.Sales)
abline(fit)
install.packages("topicmodels")
install.packages("corpus.JSS.papers", repos = "http://datacube.wu.ac.at/", type = "source")
data("JSS_papers", package = "corpus.JSS.papers")
JSS_papers <- JSS_papers[JSS_papers[,"date"] < "2010-08-05",]
JSS_papers <- JSS_papers[sapply(JSS_papers[, "description"], Encoding) == "unknown",
)
JSS_papers <- JSS_papers[sapply(JSS_papers[, "description"], Encoding) == "unknown",]
dim(JSS_papers)
library("tm")
library("XML")
remove_HTML_markup <- function(s) tryCatch({
doc <- htmlTreeParse(paste("<!DOCTYPE html>", s),
asText = TRUE, trim = FALSE)
install.packages("XML")
install.packages("XML")
remove_HTML_markup <- function(s) tryCatch({
doc <- htmlTreeParse(paste("<!DOCTYPE html>", s),
asText = TRUE, trim = FALSE)
remove_HTML_markup <- function(s) tryCatch({
doc <- htmlTreeParse(paste("<!DOCTYPE html>", s),
asText = TRUE, trim = FALSE)
xmlValue(xmlRoot(doc))
}, error = function(s) s)
corpus <- Corpus(VectorSource(sapply(JSS_papers[, "description"], remove_HTML_markup))
)
install.packages(c('rzmq','repr','IRkernel','IRdisplay'),
repos = c('http://irkernel.github.io/', getOption('repos')))
IRkernel::installspec()
IRkernel::installspec()
library(RMySQL)
install.packages("RMySQL")
library(RMySQL)
?dbConnect
if (require("RSQLite")) {
# SQLite only needs a path to the database. Other database drivers
# will require more details (like username, password, host, port etc)
con <- dbConnect(RSQLite::SQLite(), ":memory:")
con
dbListTables(con)
dbDisconnect(con)
}
library(RMySQL) # will load DBI as well
## open a connection to a MySQL database
username <- "irina"
password <- "KrcT4FJ"
con <- dbConnect(RMySQL::MySQL(),
user=username,
passwd=password,
host="dev.kovits.com",
port=3306,
dbname="findmine"
)
?dbConnect
con <- dbConnect(RMySQL::MySQL(),
user=username,
password=password,
host="dev.kovits.com",
port=3306,
dbname="findmine"
)
## list the tables in the database
dbListTables(con)
res <- dbSendQuery(con, "SELECT * FROM data_items limit 10000")
head(res)
res
?sample
dim(JSS_papers)
JSS_papers[1,1]
JSS_papers[, "desciption"]
library(tm)
?VectorSource
docs <- c("This is a text.", "This another one.")
(vs <- VectorSource(docs))
vs
inspect(VCorpus(vs))
install.packages("SnowballC")
?DocumentTermMatrix
1.5+2.5+.75+.75+1.52.5+2.25+2+2.25+1
1.5+2.5+.75+.75+1.5+2.5+2.25+2+2.25+1
135*.6
81*.8
65*40*5
65*40*52
135000/40/48
3212.69+4891.81
3212.69+4891.81+139.91
3212.69+4891.81+139.91-752.35
3212.69+4891.81+139.91-752.35-25.03
3212.69+4891.81+139.91-752.35-25.03-339.82
3212.69+4891.81+139.91-752.35-25.03-339.82-65
3212.69+4891.81+139.91-752.35-25.03-339.82-65-50
5.32/78.93
5.32/(78.93-1998.)
5.32/(78.93-19.98)
29.99*1.09+29.14)1.09
29.99*1.09+29.14*1.09+19.98
29.99*1.09+29.14*1.09+19.80
29.99*1.09+29.14*1.09
120*20
install.packages("Rtools")
256*256
30000*2
x <- -10:10
y<- -10:10
(x-min(x))/max(x)
151.8*0.005
data(geyser, package="MASS")
x=geyser$duration
est <- bkde(x, bandwidth=0.25)
installpackages("KernSmooth")
install.packages("KernSmooth")
library(KernSmooth)
est <- bkde(x, bandwidth=0.25)
plot(est, type="l")
data(geyser, package="MASS")
x <- cbind(geyser$duration, geyser$waiting)
est <- bkde2D(x, bandwidth=c(0.7, 7))
contour(est$x1, est$x2, est$fhat)
persp(est$fhat)
image(est$x1, est$x2, est$fhat)
getwd()
getwd()
install.packages("Rserve")
library(Rserve)
?Rserve
Rserve()
Rserve()
Rserve(args="--no-save")
library(Rserve)
Rserve()
?Rserve
librar(Rserve)
library(Rserve)
Rserve()
14.99*1.09
14.99*1.09*2
5*1.09
2.09+5.45+32.69
6*8
5*8*(1-.015))1.09
5*8*(1-.015))1.09
5*8*(1-.015)*1.09
8*(1-.015)*1.09
2.19+7.99+3.73+3.99+1.99
(2.19+7.99+3.73+3.99+1.99)*(1-.015)
42.95+8.59+19.59
67.89*.015
19.89*.015
(19.89-3.73)*.015
(19.89-3.73)*.015+3.73
(19.89-3.73)*(1-.015)+3.73
8*1.09
40*1.09
43.6+8.72+19.65
7.49*1.09
7.49*1.09*2
7.49*1.09*2+5.88*2
7.49*2+5.88*2
1.34/(7.49*2)
7.49*1.09*2+5.88*2 - 0.77
7.49*1.09*2-0.77/2
5.88*2-0.77/2
15.94+11.38
15.94+11.37
a=""
if(a == "")
(a == "")
Sys.getenv("R_HOME")
R.home()
.libPaths()
7012.21-68.4
7012.21-68.4-1322.99
7012.21-68.4-1322.99-140
7012.21-68.4-1322.99-140+41.15
7012.21-68.4-1322.99-140+41.15+1322.99+68.40
20000*.9235*.029
5521.97+4500
5521.97+4500+1322.99
4500/2
5521.97+4500+1322.99-2250
5521.97+4500-2250
10021.97-2250
7771.97-2250
1/4666.67
1/573.77
2916.67/70000
625/7000
625/70000
625/70000*100
getwd()
library(caret)
?trainControl
?plot
list(
c(8,NA,8),
c(7,7,3),
10,
c(3,4,4,NA),
c(9,10,9,2)
)
c(8,7,10,3,9)
matrix(
c(
8,NA,8,
7,7,3,
3,4,4,
9,10,9,
10,10,9
),
nrow=5,
ncol=3,
byrow=TRUE
)
data<-read.table("http://www.stat.ucla.edu
/~vlew/stat130a/datasets/twins.csv",
header=TRUE, sep=",")
?read.table
install.packages("C:/Users/ikukuyeva/Documents/Software/gpclib_1.5-5.tar.gz",
repos = NULL,
type="source"
)
Sys.getenv('PATH')
filepath = "http://www.ats.ucla.edu/stat/data/test_missing_comma.txt"
### Other valid paths:
# filepath = "C:/Documents/test_missing_comma.txt"
# filepath = "./test_missing_comma.txt"
data <- read.table(
file=filepath,
header=TRUE,
sep=","
)
summary(data)
df <- read.table(
file = filepath,
header = TRUE,
sep = ","
)
filepath = "http://www.ats.ucla.edu/stat/data/test_missing_comma.txt"
df <- read.table(
file = filepath,
header = TRUE,
sep = ","
)
df
unique(df$level)
unique(df$prgtype)
unique(df$gender)
table(df$gender, useNA='always')
### Step 0: Load required packages
library(beanplot)
library(car)
library(ggplot2)
library(lattice)
library(maps)
library(maptools)
library(rgdal)
library(plyr)
library(dplyr)
#---------------------------------
# --- Data set
#---------------------------------
df <- read.table(
"Number_of_Selected_Inpatient_Medical_Procedures__California_Hospitals__2005-2014.csv",
sep = ",",
header=TRUE,
stringsAsFactors = FALSE
)
names(df)[1] = 'Year'
### Step 1: Remove any 'statewide' counts
data_clean <- df %>%
filter( County != "STATEWIDE" )
data_LA_SF <- data_clean %>%
filter(
(County == "Los Angeles") |
(County == "San Francisco")
)
df_summary <- data_clean %>%
dplyr::group_by(Year) %>%
dplyr::summarise(Total.for.Year = sum(Volume, na.rm=TRUE))
### Step 1: Aggregate data to be at hospital level:
df_hosp <- data_clean %>%
dplyr::group_by( Latitude, Longitude ) %>%
dplyr::summarise( Total.for.Hospital = sum(Volume, na.rm=TRUE) )
### Step 2: Recode 'Volume' to have 2 categories only: high ( >100 cases ) and low ( <= 100 cases ):
df_hosp$volume_ind <- ifelse(
test = df_hosp$Total.for.Hospital > 100,
yes = 2,
no = 1 )
### Step 2: Read in shapefile and add latitude and longitude cordinates to it:
tracts = spTransform(
readOGR(
file.path("cb_2014_06_tract_500k"),
layer = "cb_2014_06_tract_500k"
),
CRS("+proj=longlat +datum=WGS84")
)
### Step 2: Preprocess data ahead of plotting
tracts@data$id = rownames(tracts@data)
tracts.points = fortify(tracts, region="id")
tracts.df = plyr::join(
tracts.points,
tracts@data,
by="id"
)
quit()
# 03/03/16
# Author: Irina Kukuyeva
# Graphics Mini-Course R Code
### Step 0: Load required packages
library(beanplot)
library(car)
library(ggplot2)
library(lattice)
library(maps)
library(maptools)
library(rgdal)
library(plyr)
library(dplyr)
#---------------------------------
# --- Data set
#---------------------------------
df <- read.table(
"Number_of_Selected_Inpatient_Medical_Procedures__California_Hospitals__2005-2014.csv",
sep = ",",
header=TRUE,
stringsAsFactors = FALSE
)
names(df)[1] = 'Year'
### Step 1: Remove any 'statewide' counts
data_clean <- df %>%
filter( County != "STATEWIDE" )
data_LA_SF <- data_clean %>%
filter(
(County == "Los Angeles") |
(County == "San Francisco")
)
df_summary <- data_clean %>%
dplyr::group_by(Year) %>%
dplyr::summarise(Total.for.Year = sum(Volume, na.rm=TRUE))
### Step 1: Aggregate data to be at hospital level:
df_hosp <- data_clean %>%
dplyr::group_by( Latitude, Longitude ) %>%
dplyr::summarise( Total.for.Hospital = sum(Volume, na.rm=TRUE) )
### Step 2: Recode 'Volume' to have 2 categories only: high ( >100 cases ) and low ( <= 100 cases ):
df_hosp$volume_ind <- ifelse(
test = df_hosp$Total.for.Hospital > 100,
yes = 2,
no = 1 )
### Step 2: Read in shapefile and add latitude and longitude cordinates to it:
tracts = spTransform(
readOGR(
file.path("cb_2014_06_tract_500k"),
layer = "cb_2014_06_tract_500k"
),
CRS("+proj=longlat +datum=WGS84")
)
### Step 2: Preprocess data ahead of plotting
tracts@data$id = rownames(tracts@data)
tracts.points = fortify(tracts, region="id")
tracts.df = plyr::join(
tracts.points,
tracts@data,
by="id"
)
setwd("~/Consulting Center/Courses/Graphics-repo/data")
# 03/03/16
# Author: Irina Kukuyeva
# Graphics Mini-Course R Code
### Step 0: Load required packages
library(beanplot)
library(car)
library(ggplot2)
library(lattice)
library(maps)
library(maptools)
library(rgdal)
library(plyr)
library(dplyr)
#---------------------------------
# --- Data set
#---------------------------------
df <- read.table(
"Number_of_Selected_Inpatient_Medical_Procedures__California_Hospitals__2005-2014.csv",
sep = ",",
header=TRUE,
stringsAsFactors = FALSE
)
names(df)[1] = 'Year'
### Step 1: Remove any 'statewide' counts
data_clean <- df %>%
filter( County != "STATEWIDE" )
data_LA_SF <- data_clean %>%
filter(
(County == "Los Angeles") |
(County == "San Francisco")
)
df_summary <- data_clean %>%
dplyr::group_by(Year) %>%
dplyr::summarise(Total.for.Year = sum(Volume, na.rm=TRUE))
### Step 1: Aggregate data to be at hospital level:
df_hosp <- data_clean %>%
dplyr::group_by( Latitude, Longitude ) %>%
dplyr::summarise( Total.for.Hospital = sum(Volume, na.rm=TRUE) )
### Step 2: Recode 'Volume' to have 2 categories only: high ( >100 cases ) and low ( <= 100 cases ):
df_hosp$volume_ind <- ifelse(
test = df_hosp$Total.for.Hospital > 100,
yes = 2,
no = 1 )
### Step 2: Read in shapefile and add latitude and longitude cordinates to it:
tracts = spTransform(
readOGR(
file.path("cb_2014_06_tract_500k"),
layer = "cb_2014_06_tract_500k"
),
CRS("+proj=longlat +datum=WGS84")
)
### Step 2: Preprocess data ahead of plotting
tracts@data$id = rownames(tracts@data)
tracts.points = fortify(tracts, region="id")
tracts.df = plyr::join(
tracts.points,
tracts@data,
by="id"
)
library(ggvis)
tracts.points %>%
ggvis(~long, ~lat) %>%
group_by(group, id) %>%
layer_paths(strokeOpacity:=0.5, stroke:=grey(0.5)) %>%
layer_points(
data=df_hosp,
x=~Longitude,
y=~Latitude,
size := input_slider(8, 20, value = 12),
fill := 'blue4') %>%
hide_legend("fill") %>%
set_options(width=400, height=400, keep_aspect=TRUE)
tracts.points %>%
ggvis(~long, ~lat) %>%
group_by(group, id) %>%
layer_paths(strokeOpacity:=0.5, stroke:=grey(0.5)) %>%
layer_points(
data=df_hosp,
x=~Longitude,
y=~Latitude,
size := input_slider(8, 20, value = 12)
) %>%
hide_legend("fill") %>%
set_options(width=400, height=400, keep_aspect=TRUE)
tracts.points %>%
ggvis(~long, ~lat) %>%
group_by(group, id) %>%
layer_paths(strokeOpacity:=0.5, stroke:=grey(0.5)) %>%
layer_points(
data=df_hosp,
x=~Longitude,
y=~Latitude,
size := input_slider(8, 100, value = 12, label='Point size')
) %>%
hide_legend("fill") %>%
set_options(width=400, height=400, keep_aspect=TRUE)
